{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c0947f",
   "metadata": {},
   "source": [
    "# Model run\n",
    "In cell three a selection can be made for the dataset, balanced or unbalanced and implementation of expert knowledge. Afterwards the code below will print the crossvalidation and test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf1f7614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "001708ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "gdf_zaanstad = gpd.read_file(\"../Data/dataset_zaanstad.gpkg\", layer=\"polluted_points\")\n",
    "gdf_oosterhout = gpd.read_file(\"../Data/dataset_oosterhout.gpkg\", layer=\"polluted_points\")\n",
    "\n",
    "# Exclude BOORPUNT_ID\n",
    "gdf_zaanstad = gdf_zaanstad.drop(columns=['BOORPUNT_ID'])\n",
    "gdf_oosterhout = gdf_oosterhout.drop(columns=['BOORPUNT_ID'])\n",
    "\n",
    "# Exclude geopandas geometry as variable\n",
    "gdf_zaanstad = gdf_zaanstad.drop(columns=['geometry'])\n",
    "gdf_oosterhout = gdf_oosterhout.drop(columns=['geometry'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e928c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "dataset = gdf_zaanstad\n",
    "# dataset = gdf_oosterhout\n",
    "\n",
    "# Normalized vs not normalized\n",
    "balanced = False\n",
    "\n",
    "# Implementation\n",
    "baseline = True\n",
    "BKK_var = False\n",
    "BKK_split = False\n",
    "\n",
    "# Different param settings for different model runs\n",
    "max_depths = {\n",
    "    'Oosterhout_unbalance' : 20,\n",
    "    'Oosterhout_balance' : 10,\n",
    "    'Zaanstad_unbalance' : 30,\n",
    "    'Zaanstad_balance' : 20,\n",
    "}\n",
    "\n",
    "if len(dataset) < 10000:\n",
    "    if not balanced:\n",
    "        max_depth = max_depths['Oosterhout_unbalance']\n",
    "    else:\n",
    "        max_depth = max_depths['Oosterhout_balance']\n",
    "else:\n",
    "    if not balanced:\n",
    "        max_depth = max_depths['Zaanstad_unbalance']\n",
    "    else:\n",
    "        max_depth = max_depths['Zaanstad_balance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "935b26d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: Zaanstad\n",
      "Dataset is not balanced\n",
      "BKK is excluded from the model\n",
      "\n",
      "Cross-validation results:\n",
      "Accuracy:  0.81953\n",
      "Recall:  0.63851\n",
      "Precision:  0.7228\n",
      "F1-score:  0.67793\n",
      "\n",
      "Test set results:\n",
      "Accuracy:  0.82403\n",
      "Recall:  0.65817\n",
      "Precision:  0.72503\n",
      "F1-score:  0.65817\n"
     ]
    }
   ],
   "source": [
    "# Model print statement\n",
    "if len(dataset) > 10000:\n",
    "    datasetName = 'Zaanstad'\n",
    "    print(f'Dataset: {datasetName}')\n",
    "else:\n",
    "    datasetName = 'Oosterhout'\n",
    "    print(f'Dataset: {datasetName}')\n",
    "    \n",
    "if balanced:\n",
    "    print('Dataset is balanced')\n",
    "else:\n",
    "    print('Dataset is not balanced')\n",
    "    \n",
    "if baseline:\n",
    "    print('BKK is excluded from the model')\n",
    "if BKK_var:\n",
    "    print('BKK is used as variable in the model')\n",
    "if BKK_split:\n",
    "    print('The model is split into three using the BKK')\n",
    "\n",
    "# BASELINE MODEL (Excluding BKK)\n",
    "# --------------------------------------------------------------------------------\n",
    "if baseline:\n",
    "    # Exclude BKK\n",
    "    gdf_baseline = dataset.drop(columns=['BKK'])\n",
    "\n",
    "    # Encode the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    gdf_baseline['TOETS_WBB'] = label_encoder.fit_transform(gdf_baseline['TOETS_WBB'])\n",
    "    label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "\n",
    "    # Define features and target variable\n",
    "    X = gdf_baseline.drop(columns=['TOETS_WBB'])\n",
    "    y = gdf_baseline['TOETS_WBB']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Columns to normalize\n",
    "    columns_to_normalize = ['days_since_ref', 'X', 'Y']\n",
    "\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform both training and testing data\n",
    "    X_train[columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n",
    "    X_test[columns_to_normalize] = scaler.transform(X_test[columns_to_normalize])\n",
    "\n",
    "    # Initialize the Random Forest model\n",
    "    if balanced:\n",
    "        rf_model = RandomForestClassifier(n_estimators=200, max_depth=max_depth, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    else:\n",
    "        rf_model = RandomForestClassifier(n_estimators=200, max_depth=max_depth, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Perform 10-fold cross-validation\n",
    "    scores = cross_validate(rf_model, X_train, y_train, cv=10, scoring=['accuracy', 'precision', 'recall', 'f1'], return_train_score=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    print(\"Accuracy: \", round(scores['test_accuracy'].mean(), 5))\n",
    "    print(\"Recall: \", round(scores['test_recall'].mean(), 5))\n",
    "    print(\"Precision: \", round(scores['test_precision'].mean(), 5))\n",
    "    print(\"F1-score: \", round(scores['test_f1'].mean(), 5))\n",
    "    \n",
    "    # Train the model on the entire training set\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred)\n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print the test set results\n",
    "    print(\"\\nTest set results:\")\n",
    "    print(\"Accuracy: \", round(test_accuracy, 5))\n",
    "    print(\"Recall: \", round(test_recall, 5))\n",
    "    print(\"Precision: \", round(test_precision, 5))\n",
    "    print(\"F1-score: \", round(test_recall, 5))\n",
    "    \n",
    "    # Safe results to excel\n",
    "    # Define the path to your existing CSV file\n",
    "    csv_file_path = 'results.csv'\n",
    "    new_row = {\n",
    "        'name': f'{datasetName}__Balanced:{balanced}__Baseline:{baseline}__VAR:{BKK_var}__SPLIT:{BKK_split}',\n",
    "        'Accuracy_cross-validation': round(scores['test_accuracy'].mean(), 5),\n",
    "        'AccuracyTestvalidation': round(test_accuracy, 5),\n",
    "        'RecallCross-validation': round(scores['test_recall'].mean(), 5),\n",
    "        'RecallTest validation': round(test_recall, 5),\n",
    "        'PrecisionCross-validation': round(scores['test_precision'].mean(), 5),\n",
    "        'PrecisionTest validation': round(test_precision, 5),\n",
    "        'F1 scoreCross-validation': round(scores['test_f1'].mean(), 5),\n",
    "        'F1 scoreTest validation': round(test_recall, 5)\n",
    "    }\n",
    "\n",
    "    df_new_row = pd.DataFrame([new_row])\n",
    "    df_new_row.to_csv(csv_file_path, mode='a', index=False, header=False)\n",
    "    \n",
    "# Model Including BKK as independent variable\n",
    "# --------------------------------------------------------------------------------\n",
    "if BKK_var:\n",
    "    # Define the mapping for ordinal encoding\n",
    "    bkk_mapping = {'AW_2000': 1, 'Wonen': 2, 'Industrie': 3}\n",
    "\n",
    "    # Apply the mapping to the BKK column\n",
    "    gdf_bkk_variable = dataset\n",
    "    gdf_bkk_variable = gdf_bkk_variable.replace({\"BKK\": bkk_mapping})\n",
    "    gdf_bkk_variable = gdf_bkk_variable[gdf_bkk_variable['BKK'] != 'Onbekend']\n",
    "    gdf_bkk_variable = gdf_bkk_variable.dropna()\n",
    "        \n",
    "    # Encode the target variable\n",
    "    label_encoder = LabelEncoder()\n",
    "    gdf_bkk_variable['TOETS_WBB'] = label_encoder.fit_transform(gdf_bkk_variable['TOETS_WBB'])\n",
    "\n",
    "    # Define features and target variable\n",
    "    X = gdf_bkk_variable.drop(columns=['TOETS_WBB'])\n",
    "    y = gdf_bkk_variable['TOETS_WBB']\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "    # Columns to normalize\n",
    "    columns_to_normalize = ['days_since_ref', 'X', 'Y']\n",
    "\n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform both training and testing data\n",
    "    X_train[columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n",
    "    X_test[columns_to_normalize] = scaler.transform(X_test[columns_to_normalize])\n",
    "\n",
    "    # Initialize the Random Forest model\n",
    "    if balanced:\n",
    "        rf_model = RandomForestClassifier(n_estimators=200, max_depth=max_depth, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "    else:\n",
    "        rf_model = RandomForestClassifier(n_estimators=200, max_depth=max_depth, random_state=42, n_jobs=-1)\n",
    "\n",
    "    # Perform 10-fold cross-validation\n",
    "    scores = cross_validate(rf_model, X_train, y_train, cv=10, scoring=['accuracy', 'precision', 'recall', 'f1'], return_train_score=True)\n",
    "\n",
    "    # Print the results\n",
    "    print(\"\\nCross-validation results:\")\n",
    "    print(\"Accuracy: \", round(scores['test_accuracy'].mean(), 5))\n",
    "    print(\"Recall: \", round(scores['test_recall'].mean(), 5))\n",
    "    print(\"Precision: \", round(scores['test_precision'].mean(), 5))\n",
    "    print(\"F1-score: \", round(scores['test_f1'].mean(), 5))\n",
    "    \n",
    "    # Train the model on the entire training set\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the test set\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "    test_precision = precision_score(y_test, y_pred)\n",
    "    test_recall = recall_score(y_test, y_pred)\n",
    "    test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # Print the test set results\n",
    "    print(\"\\nTest set results:\")\n",
    "    print(\"Accuracy: \", round(test_accuracy, 5))\n",
    "    print(\"Recall: \", round(test_recall, 5))\n",
    "    print(\"Precision: \", round(test_precision, 5))\n",
    "    print(\"F1-score: \", round(test_recall, 5))\n",
    "    \n",
    "    # Safe results to excel\n",
    "    # Define the path to your existing CSV file\n",
    "    csv_file_path = 'results.csv'\n",
    "    new_row = {\n",
    "        'name': f'{datasetName}__Balanced:{balanced}__Baseline:{baseline}__VAR:{BKK_var}__SPLIT:{BKK_split}',\n",
    "        'Accuracy_cross-validation': round(scores['test_accuracy'].mean(), 5),\n",
    "        'AccuracyTestvalidation': round(test_accuracy, 5),\n",
    "        'RecallCross-validation': round(scores['test_recall'].mean(), 5),\n",
    "        'RecallTest validation': round(test_recall, 5),\n",
    "        'PrecisionCross-validation': round(scores['test_precision'].mean(), 5),\n",
    "        'PrecisionTest validation': round(test_precision, 5),\n",
    "        'F1 scoreCross-validation': round(scores['test_f1'].mean(), 5),\n",
    "        'F1 scoreTest validation': round(test_recall, 5)\n",
    "    }\n",
    "\n",
    "    df_new_row = pd.DataFrame([new_row])\n",
    "    df_new_row.to_csv(csv_file_path, mode='a', index=False, header=False)\n",
    "    \n",
    "# Models based on BKK split\n",
    "# --------------------------------------------------------------------------------\n",
    "if BKK_split:\n",
    "    gdf_bkk_aw2000 = dataset[dataset['BKK'] == 'AW_2000']\n",
    "    gdf_bkk_wonen = dataset[dataset['BKK'] == 'Wonen']\n",
    "    gdf_bkk_industrie = dataset[dataset['BKK'] == 'Industrie']\n",
    "\n",
    "    datasets = {\n",
    "        'AW_2000' : gdf_bkk_aw2000,\n",
    "        'Wonen' : gdf_bkk_wonen,\n",
    "        'Industrie' : gdf_bkk_industrie\n",
    "    }\n",
    "    \n",
    "    for datasetName, data in datasets.items():\n",
    "        print(f\"\\nResults for {datasetName}:\")\n",
    "        print(\"-----------------------------\")\n",
    "        \n",
    "        # Exclude BKK\n",
    "        data = data.drop(columns=['BKK'])\n",
    "\n",
    "        # Encode the target variable\n",
    "        label_encoder = LabelEncoder()\n",
    "        data['TOETS_WBB'] = label_encoder.fit_transform(data['TOETS_WBB'])\n",
    "\n",
    "        # Define features and target variable\n",
    "        X = data.drop(columns=['TOETS_WBB'])\n",
    "        y = data['TOETS_WBB']\n",
    "\n",
    "        # Split the data into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "        # Columns to normalize\n",
    "        columns_to_normalize = ['days_since_ref', 'X', 'Y']\n",
    "\n",
    "        # Initialize the scaler\n",
    "        scaler = StandardScaler()\n",
    "\n",
    "        # Fit the scaler on the training data and transform both training and testing data\n",
    "        X_train[columns_to_normalize] = scaler.fit_transform(X_train[columns_to_normalize])\n",
    "        X_test[columns_to_normalize] = scaler.transform(X_test[columns_to_normalize])\n",
    "\n",
    "        # Initialize the Random Forest model\n",
    "        if balanced:\n",
    "            rf_model = RandomForestClassifier(n_estimators=200, max_depth=max_depth, random_state=42, n_jobs=-1, class_weight='balanced')\n",
    "        else:\n",
    "            rf_model = RandomForestClassifier(n_estimators=200, max_depth=max_depth, random_state=42, n_jobs=-1)\n",
    "\n",
    "        # Perform 10-fold cross-validation\n",
    "        scores = cross_validate(rf_model, X_train, y_train, cv=10, scoring=['accuracy', 'precision', 'recall', 'f1'], return_train_score=True)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Cross-validation results:\")\n",
    "        print(\"Accuracy: \", round(scores['test_accuracy'].mean(), 5))\n",
    "        print(\"Recall: \", round(scores['test_recall'].mean(), 5))\n",
    "        print(\"Precision: \", round(scores['test_precision'].mean(), 5))\n",
    "        print(\"F1-score: \", round(scores['test_f1'].mean(), 5))\n",
    "\n",
    "        # Train the model on the entire training set\n",
    "        rf_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the test set\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "\n",
    "        # Evaluate the model on the test set\n",
    "        test_accuracy = accuracy_score(y_test, y_pred)\n",
    "        test_precision = precision_score(y_test, y_pred)\n",
    "        test_recall = recall_score(y_test, y_pred)\n",
    "        test_f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "        # Print the test set results\n",
    "        print(\"\\nTest set results:\")\n",
    "        print(\"Accuracy: \", round(test_accuracy, 5))\n",
    "        print(\"Recall: \", round(test_recall, 5))\n",
    "        print(\"Precision: \", round(test_precision, 5))\n",
    "        print(\"F1-score: \", round(test_recall, 5))\n",
    "        \n",
    "        # Safe results to excel\n",
    "        # Define the path to your existing CSV file\n",
    "        csv_file_path = 'results.csv'\n",
    "        new_row = {\n",
    "            'name': f'{datasetName}__Balanced:{balanced}__Baseline:{baseline}__VAR:{BKK_var}__SPLIT:{BKK_split}__Type:{datasetName}',\n",
    "            'Accuracy_cross-validation': round(scores['test_accuracy'].mean(), 5),\n",
    "            'AccuracyTestvalidation': round(test_accuracy, 5),\n",
    "            'RecallCross-validation': round(scores['test_recall'].mean(), 5),\n",
    "            'RecallTest validation': round(test_recall, 5),\n",
    "            'PrecisionCross-validation': round(scores['test_precision'].mean(), 5),\n",
    "            'PrecisionTest validation': round(test_precision, 5),\n",
    "            'F1 scoreCross-validation': round(scores['test_f1'].mean(), 5),\n",
    "            'F1 scoreTest validation': round(test_recall, 5)\n",
    "        }\n",
    "\n",
    "        df_new_row = pd.DataFrame([new_row])\n",
    "        df_new_row.to_csv(csv_file_path, mode='a', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f79f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
